The PylabPraxis backend is a sophisticated Python-based platform, built with FastAPI, designed to automate laboratory workflows. It focuses on orchestrating experimental protocols, managing physical and logical laboratory assets, and maintaining a real-time interface with PyLabRobot-controlled hardware.

### **High-Level Backend Overview**

Core Architectural Principles:

* API-First Design: The backend exposes a comprehensive REST API via FastAPI, enabling seamless interaction with the Flutter-based frontend and other external systems.
* Modular Componentry: Logic is strictly separated into distinct core components (Orchestrator, AssetManager, WorkcellRuntime, Workcell) with clearly defined responsibilities, promoting maintainability and scalability.
* Database-Centric Data Management: PostgreSQL serves as the primary persistent data store for all definitions, logs, and asset states. Data access is abstracted through a dedicated service layer utilizing SQLAlchemy ORMs for robust and consistent interactions.
* PyLabRobot Integration: Deep integration with the PyLabRobot library allows for direct control and abstraction of various laboratory instruments and resources.
* Containerization: The entire backend ecosystem is designed for deployment using Docker and Docker Compose, facilitating isolated and reproducible environments.
* Authentication & Authorization: Keycloak is integrated to manage user identities and access control.

Key Backend Components & Their Roles:

1. FastAPI Application & API Layer (main.py, backend/api/)
   * main.py: Serves as the application's entry point, handling FastAPI initialization, global logging configuration, database schema setup (within its lifespan function), and the instantiation of critical singleton services like the Orchestrator.
   * backend/api/: Contains various FastAPI routers (protocols.py, assets.py, workcell\_api.py) that define and group API endpoints. These endpoints manage protocol discovery and execution, asset querying and modification, and real-time workcell state monitoring. They rely on dependency injection (backend/api/dependencies.py) to securely access database sessions and core business logic components.
2. Core Business Logic (backend/core/)
   * Orchestrator (orchestrator.py): This is the central brain for protocol execution. It is responsible for fetching protocol definitions from the database, preparing the execution environment (including PraxisState for run-specific data), injecting required assets (by interacting with AssetManager), and executing the Python functions defined as protocols. It also manages the comprehensive logging of protocol runs and individual function calls to the database and processes control commands (e.g., pause, resume, cancel) during execution.
   * AssetManager (asset\_manager.py): Manages the entire lifecycle and allocation of physical laboratory assets. It serves as the primary interface for acquiring and releasing machines and resources for protocol runs, ensuring proper locking and status updates in the database. AssetManager leverages dedicated data services for its database interactions and coordinates directly with WorkcellRuntime to activate and deactivate the live PyLabRobot hardware instances. It also includes functionality to synchronize PyLabRobot definitions with the application's asset catalog in the database.
   * WorkcellRuntime (workcell\_runtime.py): Manages the *live, operational instances* of PyLabRobot objects. It's responsible for dynamically instantiating PyLabRobot Machine and Resource objects based on database definitions (e.g., FQNs and configuration parameters). It performs the actual setup() and stop() calls on these objects to connect/disconnect from hardware. WorkcellRuntime holds references to the primary Workcell instance (which contains the base PLR objects) and facilitates their placement on simulated or physical decks. It also provides structured representations of the live deck state for frontend visualization.
   * Workcell (workcell.py): This class serves as a clean, high-level container for *all configured PyLabRobot objects* (machines and resources) within the lab setup. It is primarily responsible for holding these PLR instances in memory (potentially loaded from configuration files) and managing their internal state (e.g., content levels, positions) through PyLabRobot's serialization mechanisms. It offers methods for saving and loading the entire workcell's state (including nested resource states) to/from JSON files and for continuous backup. Crucially, Workcell does not directly manage asset allocation status in the database or implement asset locking; these responsibilities are delegated to AssetManager and WorkcellRuntime.
   * Deck Management (New Approach): The dedicated Deck class in backend/core/deck.py is being deprecated. Instead, deck layouts will be handled through:
     * Dynamic Inference: A utility to infer deck layouts on the fly based on protocol arguments.
     * User File Uploads: Allowing users to upload custom deck layout files.
     * In-Function Layout Generation with @deck\_layout Decorator: Enabling programmatic definition of deck layouts directly within protocol files using a new @deck\_layout decorator.
   * Protocol Definition & Discovery: This crucial aspect of the backend enables the system to understand and manage executable laboratory protocols:
     * @protocol\_function Decorator: Developers define protocols as standard Python functions, annotated with this decorator (e.g., in backend/protocol\_core/decorators.py). This decorator is used to embed rich metadata (name, version, description, parameters with type hints and constraints, required assets, execution flags like is\_top\_level) directly within the function's code. It also wraps the original function to inject execution context (PraxisRunContext) and enable automatic logging of function calls.
     * ProtocolDiscoveryService: This service (in backend/protocol\_core/discovery\_service.py) is responsible for scanning configured Python code sources (Git repositories or local file system paths). It identifies @protocol\_function decorated functions, extracts their embedded metadata, and for undecorated functions, attempts to infer basic protocol information from their signatures. This collected metadata is then converted into structured Pydantic models.
     * FunctionProtocolDefinitionOrm (ORM Models): The structured metadata extracted by the ProtocolDiscoveryService is persisted into the PostgreSQL database using the FunctionProtocolDefinitionOrm SQLAlchemy ORM model (defined in backend/models/protocol\_definitions\_orm.py). This model, along with related ParameterDefinitionOrm and AssetDefinitionOrm, stores a canonical, static definition of each discoverable protocol.
     * Orchestrator's Role in Inspection & Execution: When a protocol run is initiated, the Orchestrator retrieves the relevant FunctionProtocolDefinitionOrm from the database. Its \_prepare\_protocol\_code method dynamically loads the Python module and accesses the actual function (which retains its protocol\_metadata attribute from the decorator). The \_prepare\_arguments method then uses this live metadata and the function's signature to validate user input, resolve and acquire necessary assets via AssetManager, and prepare the final arguments for the protocol function's execution.
   * Protocol Run Context & State Management (backend/core/run\_context.py, backend/utils/state.py):
     * PraxisState: This class (from backend/utils/state.py) represents the *canonical, mutable shared state* for a single top-level protocol run. It is designed to hold all experimental parameters, intermediate results, and tracking information that needs to persist across different steps or function calls within a run. It is backed by Redis for efficient runtime in-memory access and persistence, allowing for robust state management even across application restarts (for pause/resume scenarios). The Orchestrator manages the lifecycle of a PraxisState instance for each run.
     * PraxisRunContext: This immutable object (from backend/core/run\_context.py) is the central carrier of essential execution and logging information that is passed down the call stack of @protocol\_function calls. For each function call, the PraxisRunContext provides:
       * run\_guid: The unique identifier of the top-level protocol run.
       * protocol\_run\_db\_id: The database ID of the ProtocolRunOrm entry.
       * function\_definition\_db\_id: The database ID of the currently executing FunctionProtocolDefinitionOrm.
       * parent\_function\_call\_log\_id: The database ID of the calling function's log entry (for building the call hierarchy).
       * db\_session: The SQLAlchemy session for database interactions within the function's scope (e.g., for logging).
       * canonical\_state: A reference to the mutable PraxisState object for the current run.
         The @protocol\_function decorator is responsible for creating and propagating updated PraxisRunContext instances to maintain the correct call hierarchy and context for nested protocol function calls.
3. Data Service Layer (backend/services/)
   * This package provides a crucial abstraction layer, encapsulating all direct database interactions. Instead of direct ORM manipulation in business logic, components interact with dedicated service classes.
   * protocol\_data\_service.py: Manages CRUD operations for protocol definitions, protocol run instances, and detailed function call logs.
   * asset\_data\_service.py: Handles broad asset-related data, including resource definitions (ResourceDefinitionCatalogOrm) and resource instances (ResourceInstanceOrm). It provides methods for creating, retrieving, updating, and listing these assets.
   * machine\_data\_service.py: Specializes in managing machine-specific data, including MachineOrm entries. It provides methods for CRUD operations on machine definitions, tracking their status (MachineStatusEnum), and managing their association with protocol runs.
   * workcell\_data\_service.py: Focuses on managing data related to the overall workcell configuration, including WorkcellOrm and potentially other high-level workcell metadata. It provides methods for defining and retrieving workcell-specific settings and configurations.
   * deck\_data\_service.py: Handles the persistent storage and retrieval of DeckLayoutOrm and DeckSlotOrm models, managing predefined deck configurations.
   * praxis\_orm\_service.py: Offers a more generalized interface for common database operations, used by various parts of the system.
   * All these services utilize SQLAlchemy ORM models defined in backend/models/ to interact with the PostgreSQL database.
4. Utility Modules (backend/utils/)
   * This package contains various helper modules that provide common functionalities and abstractions used across the backend.
   * db.py: This foundational module handles all database connectivity setup for the PostgreSQL database. It defines the SQLAlchemy async\_engine, AsyncSessionLocal (for creating asynchronous sessions), the Base class for ORM models, and the init\_praxis\_db\_schema() function (which is called during application startup within main.py's lifespan function to create database tables). It also manages the loading of database connection strings from configuration.
   * errors.py: Defines custom exception classes (e.g., WorkcellRuntimeError, AssetAcquisitionError, ProtocolCancelledError). These exceptions provide a structured way to handle specific error conditions within the application, allowing for more precise error logging, reporting, and recovery mechanisms.
   * logging.py: Provides a centralized and standardized approach to logging within the backend. It offers a get\_logger function for consistent logger instantiation and includes powerful decorators (log\_async\_runtime\_errors, log\_runtime\_errors) that automatically catch, log, and optionally re-raise exceptions from both asynchronous and synchronous functions. This ensures consistent error reporting and debugging across the application.
   * notify.py: Provides utilities for sending various types of notifications (e.g., email, SMS). This module can be integrated into protocols or the Orchestrator to alert users about protocol status changes, errors, or other important events.
   * plr\_inspection.py: Contains helper functions for introspecting PyLabRobot (PLR) classes and objects. It's used by the AssetManager (specifically in sync\_pylabrobot\_definitions) to extract metadata (like constructor parameters, resource dimensions, categories) from PLR objects, which is then used to populate the ResourceDefinitionCatalogOrm in the database.
   * redis\_lock.py: Implements a distributed locking mechanism using Redis. This utility is crucial for managing concurrent access to shared resources across different backend processes or protocol runs, preventing race conditions and ensuring data integrity. It can be used by AssetManager or Orchestrator for fine-grained control over critical sections.
   * run\_control.py: Provides functions for sending and retrieving control commands (e.g., "PAUSE", "RESUME", "CANCEL") to a running protocol. These commands are typically communicated via Redis and are actively monitored and acted upon by the Orchestrator to manage the lifecycle of a protocol run.

Key Database Interaction Points:

* Application Startup (main.py): Ensures the database schema is initialized and all necessary tables are created via init\_praxis\_db\_schema() (called from main.py's lifespan function).
* Protocol Execution (Orchestrator): Persistently logs protocol runs and individual function calls, updating their statuses and storing input/output data and state snapshots using protocol\_data\_service.
* Asset Management (AssetManager, WorkcellRuntime): AssetManager drives the updates to MachineOrm (via machine\_data\_service) and ResourceInstanceOrm (via asset\_data\_service) statuses (e.g., IN\_USE, AVAILABLE) and manages asset definitions (ResourceDefinitionCatalogOrm) through relevant data services. WorkcellRuntime updates machine and resource instance statuses based on their live operational state and fetches necessary configuration from the database during instantiation.
* API Endpoints (backend/api/): All API calls needing persistent data interaction utilize injected AsyncSession objects and rely on the data service layer to perform queries and updates.